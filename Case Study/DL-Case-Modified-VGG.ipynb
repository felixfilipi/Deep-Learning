{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "853c64f8",
   "metadata": {},
   "source": [
    "<h3 align=\"right\">Felix Filipi</h3>\n",
    "<h3 align=\"right\">2301877590</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061f5fd",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Ujian Tengah Semester</h1>\n",
    "<h1 align=\"center\">Deep Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d759f5",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<h1>2.</h1>\n",
    "\n",
    "[LO 1, LO 2, LO 3 & LO 4, 40 points] You are asked to create a system that can classify if an image\n",
    "is a cat or dog (image binary classification). The dataset to train the network can be accessed at\n",
    "Cats-vs-Dogs Kaggle dataset https://www.kaggle.com/shaunthesheep/microsoft-catsvsdogs-dataset\n",
    "To complete this task, you need to:\n",
    "\n",
    "a. [5 points] Create an image classification program with a custom CNN architecture. Please\n",
    "give attention the resolution of image dataset of your input. Submit the code in Python\n",
    "Notebook.\n",
    "\n",
    "b. [20 points] Try to reach the best accuracy by designing the proper CNN architecture and\n",
    "by tuning all hyperparameter including batch normalization and drop-out. Explain the\n",
    "uniqueness of your CNN architecture.\n",
    "\n",
    "c. [15 points] Submit the result of your CNN training. Then, analyze and explain the result.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366733b",
   "metadata": {},
   "source": [
    "We are asked to create a custom binary classification of cat and dog using Convolutional Neural Network (CNN) architecture. Here I will use keras framework from tensorflow to make programming easier.\n",
    "\n",
    "<b>First, we have to import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1d4408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    ")\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974c1e3",
   "metadata": {},
   "source": [
    "<b>Now, we need to get our dataset, there are two ways to get the data:\n",
    "\n",
    "1. Using wget commands.\n",
    "2. Download the dataset manual.\n",
    "\n",
    "Here I've downloaded the required data manually, so we just need to extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3c13733",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_zip = 'archive.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ae327",
   "metadata": {},
   "source": [
    "<b>After that, we need to load the requested data, let's open the directory first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec7a08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '2.ipynb',\n",
       " 'archive.zip',\n",
       " 'MSR-LA - 3467.docx',\n",
       " 'PetImages',\n",
       " 'readme[1].txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93bc11a",
   "metadata": {},
   "source": [
    "<b> Go into the PetImages dir, which is our work directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3539b36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cat', 'Dog']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = 'PetImages'\n",
    "os.listdir(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087beff1",
   "metadata": {},
   "source": [
    "<b>We already know the required working directory. But before inserting it into our neural network model, we need to find out do all data have the same size? Therefore, we need to check the size of each image, to do that, let's define several directory first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd5a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_path = os.path.join(base_dir,'Cat')\n",
    "dog_path = os.path.join(base_dir,'Dog')\n",
    "cat_items = os.listdir(cat_path)\n",
    "dog_items = os.listdir(cat_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48800d13",
   "metadata": {},
   "source": [
    "<b>Open some image to check do all image in this set have the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab47f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 281) (312, 397)\n"
     ]
    }
   ],
   "source": [
    "img_1 = Image.open(os.path.join(cat_path,'1.jpg'))\n",
    "img_2 = Image.open(os.path.join(cat_path,'2.jpg'))\n",
    "print(img_1.size,img_2.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e594c",
   "metadata": {},
   "source": [
    "<b> Now we know that each images in this set have a different size each other. So, our job is to resize all the images in that directory so they have the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c01b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New directory for images\n",
    "new_cat_path = os.path.join('dataset/','cat/')\n",
    "new_dog_path = os.path.join('dataset/','dog/')\n",
    "\n",
    "def resize(dirs,path,new_path):\n",
    "    \n",
    "    # Create new directory\n",
    "    os.makedirs(new_path)\n",
    "    \n",
    "    print('Please wait a moment!')\n",
    "    \n",
    "    \n",
    "    # *Note: I need to reduce the data amount as well, because to allocate data matrix\n",
    "    # need larger RAM / memory than my laptop capability. So, we need to reduce dataset\n",
    "    # in order to execute the model.\n",
    "    \n",
    "    # Using try catch in case pillow cant read the image (return unidentified image)\n",
    "    # this process will remove several unindentified image too, so we don't need to reduce\n",
    "    # the image manually.\n",
    "    \n",
    "    try:\n",
    "        for item in dirs:\n",
    "            img = Image.open(path+'/'+item)\n",
    "\n",
    "            # Sometimes return error cannot write mode P as JPEG\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "\n",
    "            imResize = img.resize((300,300), Image.ANTIALIAS)\n",
    "            imResize.save(new_path + item , 'JPEG', quality=90)\n",
    "            \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc07f57",
   "metadata": {},
   "source": [
    "So, what are we doing here? We want to equalize the image size for each picture so that it can be fed into our neural network model later. Several things to note in the code above:\n",
    "\n",
    "1. Make sure that the resize directory does not exist in your set when this function is run.\n",
    "2. This process will take a long time, considering that we have thousands of data.\n",
    "3. Sometimes images cannot be read and are considered as unidentified images, so we use try except to avoid this error. There are not many unidentified images, so they are safe to throw away.\n",
    "4. Sometimes the image can't be saved, and returns error \"cannot write mote P as ... \", therefore we need to convert it to RGB image so as to get rid of this error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342372a",
   "metadata": {},
   "source": [
    "<b> Now, let's call the resize function to our cat dataset, since it has about 12k images, the process would take several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1ceca8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait a moment!\n"
     ]
    }
   ],
   "source": [
    "resize(cat_items,cat_path,new_cat_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843caa98",
   "metadata": {},
   "source": [
    "<b> Now, let's call the resize function to our dog dataset, the process would take several times. So, you can drink a coffe first :')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a98c72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait a moment!\n"
     ]
    }
   ],
   "source": [
    "resize(dog_items,dog_path,new_dog_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5cdd80",
   "metadata": {},
   "source": [
    "<b> Now, let's try to check the size of image again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb6a1bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300) (300, 300)\n"
     ]
    }
   ],
   "source": [
    "new_cat_path = os.path.join('dataset/','cat/')\n",
    "new_dog_path = os.path.join('dataset/','dog/')\n",
    "img_1 = Image.open(os.path.join(new_cat_path,'1.jpg'))\n",
    "img_2 = Image.open(os.path.join(new_cat_path,'2.jpg'))\n",
    "print(img_1.size,img_2.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37bf4c",
   "metadata": {},
   "source": [
    "<b> Yash!! We did it, now go to the model section.</b>\n",
    "\n",
    "Here, we are challenged to design a proper CNN architecture by setting all hyperparameters including batch normalization and dropout. Then, we should explain the uniqueness of our CNN architecture.\n",
    "\n",
    "So in order to answer it. I want to show the vgg architecture which will be tweaked to show our architecture uniqueness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e7a41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', \n",
    "# padding='same', input_shape=(200, 200, 3)))\n",
    "\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "# model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2eb85a",
   "metadata": {},
   "source": [
    "VGG architecture contains:\n",
    "\n",
    "1. The one-block VGG model has a single convolutional layer with 32 filters followed by a max pooling layer.\n",
    "2. The two-block VGG model extends the one block model and adds a second block with 64 filters.\n",
    "3. The three-block VGG model extends the two block model and adds a third block with 128 filters.\n",
    "4. Fully connected layer\n",
    "\n",
    "Now, let's copy this architecture and tweak several layer in it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1639a3f",
   "metadata": {},
   "source": [
    "<b>Here's some approach that will be used in our tweak which will become our architecture uniqueness too:\n",
    "\n",
    "1. Instead of using MaxPooling layer, use stride.\n",
    "2. Batch Normalization each block\n",
    "3. Implement dropout layer for convolutional layer and fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88b98868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 22:34:58.798311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-24 22:34:59.247893: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-11-24 22:34:59.247929: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-11-24 22:34:59.249287: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-24 22:34:59.362351: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 94633984 exceeds 10% of free system memory.\n",
      "2021-11-24 22:34:59.458311: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 94633984 exceeds 10% of free system memory.\n",
      "2021-11-24 22:34:59.485003: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 94633984 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), strides = 2, activation=\"relu\", padding=\"same\",\n",
    "                 input_shape=(300, 300, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f273d3",
   "metadata": {},
   "source": [
    "<b>Now we will use binary_crossentropy as a loss function because our output is a binary classification. And use adam optimizer as backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3a55bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer = 'sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e1e02",
   "metadata": {},
   "source": [
    "Now, let's check our model summary, to ensure our architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "475a6239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 150, 150, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 150, 150, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 75, 75, 64)        18496     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 75, 75, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 75, 75, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 38, 38, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 38, 38, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 184832)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               23658624  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,752,897\n",
      "Trainable params: 23,752,449\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d481499",
   "metadata": {},
   "source": [
    "<h2> Training Process</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcfd298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8548 images belonging to 2 classes.\n",
      "Found 2137 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(validation_split=0.2)\n",
    "\n",
    "train_generator = datagen.flow_from_directory('dataset/', \n",
    "                                              batch_size=64, \n",
    "                                              target_size=(300, 300),\n",
    "                                              class_mode = 'binary', \n",
    "                                              subset = 'training',\n",
    "                                              shuffle=True)\n",
    "\n",
    "\n",
    "validation_generator = datagen.flow_from_directory('dataset/',\n",
    "                                                   batch_size=64,\n",
    "                                                   target_size=(300, 300),\n",
    "                                                   class_mode = 'binary',\n",
    "                                                   subset = 'validation',\n",
    "                                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0649c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 22:35:01.467887: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 184320000 exceeds 10% of free system memory.\n",
      "2021-11-24 22:35:01.649964: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 184320000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 327s 2s/step - loss: 0.5178 - accuracy: 0.8090 - val_loss: 0.4603 - val_accuracy: 0.8269\n",
      "Epoch 2/8\n",
      "134/134 [==============================] - 318s 2s/step - loss: 0.3435 - accuracy: 0.8616 - val_loss: 0.4159 - val_accuracy: 0.8348\n",
      "Epoch 3/8\n",
      "134/134 [==============================] - 317s 2s/step - loss: 0.2413 - accuracy: 0.9036 - val_loss: 0.3985 - val_accuracy: 0.8437\n",
      "Epoch 4/8\n",
      "134/134 [==============================] - 317s 2s/step - loss: 0.1662 - accuracy: 0.9375 - val_loss: 0.5058 - val_accuracy: 0.7656\n",
      "Epoch 5/8\n",
      "134/134 [==============================] - 317s 2s/step - loss: 0.0964 - accuracy: 0.9705 - val_loss: 0.5506 - val_accuracy: 0.8423\n",
      "Epoch 6/8\n",
      "134/134 [==============================] - 317s 2s/step - loss: 0.0585 - accuracy: 0.9844 - val_loss: 0.5921 - val_accuracy: 0.8442\n",
      "Epoch 7/8\n",
      "134/134 [==============================] - 316s 2s/step - loss: 0.0393 - accuracy: 0.9899 - val_loss: 0.6278 - val_accuracy: 0.8390\n",
      "Epoch 8/8\n",
      "134/134 [==============================] - 316s 2s/step - loss: 0.0288 - accuracy: 0.9923 - val_loss: 0.7296 - val_accuracy: 0.8465\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,epochs = 8, validation_data = validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa849975",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "Here we get an accuracy of 99,23%, which is definitely overfit model to classify this data. But, because the instruction said that we need to get the best accuracy, so let's just use it. As we can see here, in above result, the training accuracy has able to predict it with 99,23% of accuracy. Meanwhile the validation set return 84,65% of accuracy. This means that the model's overfit. This model can predict the same training set with very good accuracy, but not the same things happen on the validation / new set.\n",
    "\n",
    "## Why this is happen?\n",
    "\n",
    "This overfit happen because we create too complex architecture for this dataset. The other reason is, we just reduce the size of this dataset, and causes the model redundant to overfit. But, as I said before, this dataset must be reduced since the memory allocation of my computer not capable to load all of the set provided. I've already tried to train all dataset for the model, but my laptop's going to crash and need to be force shutdown \"twice\". This will make a lot of damage to my laptop's hardisk. So, in order to avoid this, I make a tough decision to reduce the used dataset for training. But, still the main reason is the architecture's too complex for this dataset. \n",
    "\n",
    "But I think this is not a problem since the instruction just tell us to get the best accuracy possible. That's the other reason why I create such a complex architecture :))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aedac41",
   "metadata": {},
   "source": [
    "<h2>Training accuracy and validation curves as a function of number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12dac9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA6LElEQVR4nO3dd3xUZdbA8d8hCQmBACGhhyYgHQlEiggWFLGiLoooLrgqu2B3fVdddVEs67uvq667lrVgpdrLuiAoiAWR0HtRERJaaIGQQsp5/7g3MIQJDJDJncyc7+czn5nbz0wm98x9nuc+j6gqxhhjTFnVvA7AGGNMaLIEYYwxxi9LEMYYY/yyBGGMMcYvSxDGGGP8sgRhjDHGL0sQpkKIyH9FZERFrxvKRGSkiHzrM50jIqcEsu4JHCssPjNTtUR7HYDxjojk+EzGAwVAsTv9e1WdEOi+VPXCYKx7vESkHvAm0B/YDzyrqn8L1vF8qWqtitiPiDwMtFHV4T77DtpnZkx5LEFEMN8TmohsAG5S1Zll1xORaFUtqszYTsL/AHFAYyAW6OhtOOZoqth3K+JYEZM5goicLSIZInKviGwFXheRRBH5TESyRGS3+zrFZ5vZInKT+3qkiHwrIk+56/4iIhee4LqtRGSOiOwTkZki8ryIvHOU8AuB7aqaq6q7VfW7Y7zXF0XkqTLzPhaRu93X94nIT+7xV4rIFUfZl4pIG/d1koh8IiJ7ReRHoHWZdf8hIpvc5QtEpJ87fxDwZ2CoW2S1xM9nVk1EHhSRX0Vku4i8JSJ13GUt3ThGiMhGEdkhIg8cJeaLRWSRG8cm9+rFd/mZIvK9iOxxl49059cQkb+7MWS7f8Mapd+dMvvYICLnua8fFpH3ROQdEdkLjBSRniIy1z3GFhH5l4hU99m+k4jMEJFdIrJNRP4sIo1EJFdEknzW6+5+P2PKe7/m+FiCMOVpBNQDWgCjcL4rr7vTzYE84F9H2b4XsAZIBv4GvCYicgLrTgR+BJKAh4HrjxH3fGCYiNx4jPVKTcI5GQuAiCQCA4HJ7vKfgH5AHeAR4B0RaRzAfp8H8nGuZH7nPsrG2Q3nM54IvCsicao6DXgCmKKqtVT1ND/7Huk+zgFOAWpx5N/iTKAdMAD4i4h0KCfO/cBvgbrAxcBoEbkcQERaAP8F/gnUd+Nd7G73FNADOMN9D38CSso5RlmDgffcY07AKda8C+fv38eNeYwbQwIwE5gGNAHaAF+q6lZgNnC1z36vByaramGAcZhjUVV72ANgA3Ce+/ps4AAQd5T1uwG7faZn4xRRgXPyWu+zLB5QoNHxrIuTiIqAeJ/l7wDvlBNTG2ALTv3DOuB37vxY9/3U8bONABuB/u70zcBXR3nfi4HBPrF/67NM3RiicK5k2vsse8J3XT/73Q2c5r5+uOx7LPOZfQmM8VnWzj1eNNDSjSPFZ/mPwDUBfg+eBZ5xX98PfOhnnWo4PxBO87PsbCDjKN+th4E5x4jhztLjAsOAReWsNxT4zn0dBWwFelb2/044P6wOwpQnS1XzSydEJB54BhgEJLqzE0QkSlWL/Wy/tfSFqua6P9DLq8Qtb91kYJeq5vqsuwloVs5+bgQ+UdU5IjIQ+Mbd10/AElXNLruBqqqITMY5Ec0BrsVJQqXv+7fA3TgnXt+4jqY+zsl6k8+8X31XEJF73Hib4JzQawew31JNyuzvV/d4DX3mbfV5nUs5n72I9AKeBDoD1XGS6bvu4mY4n11ZyTj1PP6WBcL3c0FETgWeBtJwfiBEAwuOEQPAx8BLItIKJ0lmq+qPJxiT8cOKmEx5ynbz+0ecf8Jeqlob51c6OL/Ag2ULUM9NTqXKSw7gnFhiAFT1F5xk9r/Aq+5zeSYBQ9wilV7A+3CwiOUV4FYgSVXrAss59nvOwrny8Y21eekLt77hTzjFI4nufrN99nusLpY34xT1+e67CNh2jO38mQh8AjRT1TrASz5xbKJM3YlrB07xmb9l+3FO8gCISBROwvRV9v29CKwG2rrfrT+XicFv02H3B8xUYDhO8dLb/tYzJ84ShAlUAk6xwh5xmpKODfYBVfVXIB14WESqi0gf4NKjbPIBTn3C5e6JaS+wBOdEllveRqq6COek9yowXVX3uItq4pzMsgBE5AacX9rHirvYjeVhEYkXkY6A7z0MCTgn9CwgWkT+gnMFUWob0FJEyvv/nATcJU4Ffi0O1VmcSGugBJyrtHwR6YlzBVVqAnCeiFwtItFuxXs3VS0BxgNPi0gTEYkSkT4iEgusBeLcyu8Y4EGcq5JjxbAXyBGR9sBon2WfAY1F5E4RiRWRBPeqp9RbOEV9l2EJosJZgjCBehaogXMi/QGn0rAyXIdTcbkTeAyYgnO/xhFUdS7OCW4szi/yOThl90OASSKSepTjTATOc59L97cS+DswF+ek3QU4aqsoH7fiFOtsBd7AqeAvNR3n81uLUzyUz+HFLqVFPDtFZKGffY/HORnOAX5xt78twLjKGgOME5F9wF9wfpEDoKobgYtwrh534dS/lFaa3wMsw6ls34VzhVbNLcYbg5NsM3GuKA5r1eTHPTh/t304V2xTfGLYB5yP88NgK07d0jk+y7/DqRxf6P6gMBVI3AoeY6oEEZkCrFbVoF/BmKpBRL4CJqrqq17HEm7sCsKENBE5XURau23/B+E0kfzI47BMiBCR04Hu+Fx1mIpjrZhMqGuEU56fhFNUMdqtMzARTkTeBC4H7nCLokwFsyImY4wxflkRkzHGGL+CVsQkIuOBS3D6xTmiaaDbtcE/cFpJ5AIjVXWhu2wETvM4gMdU9c1jHS85OVlbtmxZQdEbY0xkWLBgwQ5VLXuvChDcOog3cPqHeauc5RcCbd1HL5ybZXr5tLFPw2mDvkBEPlHV3Uc7WMuWLUlPT6+g0I0xJjKISLnNg4NWxKSqc3DaR5dnMPCWOn4A6rqdoF0AzFDVXW5SmIFzR6wxxphK5GUdRFMOvzkow51X3nxjjDGVqEpXUovIKBFJF5H0rKwsr8Mxxpiw4uV9EJkc3plZijsvE6fLYN/5s/3tQFVfBl4GSEtLO6K9bmFhIRkZGeTn5x+xrTkxcXFxpKSkEBNjY7IYE+68TBCfALe6XS33wumqd4uITAeecAduAWfwlvtP5AAZGRkkJCTQsmVLyh+rxgRKVdm5cycZGRm0atXK63CMMUEWzGauk3CuBJLdIQjHcqgr5peAz3GauK7HaeZ6g7tsl4g8itMJGMA4VT1aZXe58vPzLTlUIBEhKSkJK84zJjIELUGo6rBjLFfglnKWjcfpsfKkWXKoWPZ5GhM5rC8mY4wJUSUlyv4DRewvKCanoJB9+UXkFBSxv6Do4Ouc/CLq1arOdb1aHHuHx8kSRJDt2bOHiRMnMmbMmOPa7qKLLmLixInUrVu33HX+8pe/0L9/f84777yTjNIYU1FUlYKiEvblOyfyHJ+T+f6CIva5J/XDlxWyv6DYXVborltMTkFgY0ClNq9rCaIq2rNnDy+88MIRCaKoqIjo6PI//s8///yY+x43btxJx2eMKV/egWIydueSsTuPjN257Mg5cOjE7udEv/+AM6+o5NidoFYTqBUbTUJcDDVjo6gVG02dGjGk1K3hTsdQKy6aWj6vE2KjqRkb7W7nPNeMjaZ6dHDuWLAEEWT33XcfP/30E926dSMmJoa4uDgSExNZvXo1a9eu5fLLL2fTpk3k5+dzxx13MGrUKOBQ1yE5OTlceOGFnHnmmXz//fc0bdqUjz/+mBo1ajBy5EguueQShgwZQsuWLRkxYgSffvophYWFvPvuu7Rv356srCyuvfZaNm/eTJ8+fZgxYwYLFiwgOTnZ40/GGO/lFxYfPPk7z3lscl9nugmhrPjqzsm8lnuCrhUbTfOa8YdNH/a67HRcNAmxMcTFVAv5Or2ISRCPfLqClZv3Vug+OzapzdhLOx11nSeffJLly5ezePFiZs+ezcUXX8zy5csPNhMdP3489erVIy8vj9NPP53f/OY3JCUlHbaPdevWMWnSJF555RWuvvpq3n//fYYPH37EsZKTk1m4cCEvvPACTz31FK+++iqPPPII5557Lvfffz/Tpk3jtddeq7gPwJgQV1BUTKZ74i9NBJt8EkLWvsNHr42JEprWrUFKYjznd2xISmI8KYk13Ec8ybViiaoW2if1ihQxCSJU9OzZ87B7CJ577jk+/PBDADZt2sS6deuOSBCtWrWiW7duAPTo0YMNGzb43feVV155cJ0PPvgAgG+//fbg/gcNGkRiYqLfbY2pig4UlbB5T95hVwGbfIqEtu09PAFEVxOa1HVO+Oe2a+Cc+Os5J/9mifE0SIilWgQlgGOJmARxrF/6laVmzZoHX8+ePZuZM2cyd+5c4uPjOfvss/3e9R0bG3vwdVRUFHl5eX73XbpeVFQURUWBVW4ZE8oKi0vYsif/sHoA3yuArXvz8R3zLKqa0LhOHCmJNejftv5hVwDN6sXTsHZcRF0BnKyISRBeSUhIYN8+/6MhZmdnk5iYSHx8PKtXr+aHH36o8OP37duXqVOncu+99/LFF1+we/dRe003xjNFxSXM37Cb6Su2snLLXjJ357ElOw/f+t5qAo3r1KBpYg36tE6i2cEE4Dw3rhNHdFSV7mIupFiCCLKkpCT69u1L586dqVGjBg0bNjy4bNCgQbz00kt06NCBdu3a0bt37wo//tixYxk2bBhvv/02ffr0oVGjRiQkJFT4cYw5EfmFxXy3fgfTV2xlxspt7M4tJDa6Gl2a1qFnq3o08zn5pyTG07huHDGWACpN2IxJnZaWpmUHDFq1ahUdOnTwKKLQUFBQQFRUFNHR0cydO5fRo0ezePHik9qnfa7mZOQUFDFr9Xamr9jKrNXb2X+gmITYaAZ0aMCgzo3of2p94qvbb9fKIiILVDXN3zL7K4S5jRs3cvXVV1NSUkL16tV55ZVXvA7JRKBd+w8wc+U2pq/Yyjfrd3CgqITkWtW5rFtTBnVuRJ9TkoLWlt+cOEsQYa5t27YsWrTI6zBMBNq8J48vVmxl+optzPtlJyUKTevW4PreLRjUuRHdmydahXGIswRhjKkwP2flMM1NCks27QGgbYNa3HJOGy7o1IhOTWqH/M1h5hBLEMaYE6aqrNi8l+krtjJ9xVbWbssB4LSUOvxpUDsu6NSI1vVreRylOVGWIIwxx6W4RFm4cTfTljtJIWN3HtUEeraqx8OXdmRgp0Y0qVvD6zBNBbAEYYw5pgNFJcz9eSfTljvNUXfkFFA9qhpntk3m9nPbMqBDA5JqxR57R6ZKsWYDIaZWLedyfPPmzQwZMsTvOmeffTZlm/SW9eyzz5Kbm3tw+qKLLmLPnj0VFqcJf7kHipi2fAt3Tl5Ej8dmMGL8j3y8OJNep9Tjn8NSWfDQeYwfeTpXn97MkkOYsiuIENWkSRPee++9E97+2WefZfjw4cTHxwOBdR9uTHZuIV+u3sa05VuZsy6L/MIS6sbHMKhTIwZ1bkTfNsnExUR5HaapJJYgguy+++6jWbNm3HKLM7rqww8/THR0NLNmzWL37t0UFhby2GOPMXjw4MO227BhA5dccgnLly8nLy+PG264gSVLltC+ffvD+mIaPXo08+fPJy8vjyFDhvDII4/w3HPPsXnzZs455xySk5OZNWvWwe7Dk5OTefrppxk/3hnR9aabbuLOO+9kw4YN5XYrbsLb9r35fOHeozD3p50UlSiNascxNK0ZF3RuRM+W9az7iggVOQniv/fB1mUVu89GXeDCJ4+6ytChQ7nzzjsPJoipU6cyffp0br/9dmrXrs2OHTvo3bs3l112WbnN/1588UXi4+NZtWoVS5cupXv37geXPf7449SrV4/i4mIGDBjA0qVLuf3223n66aeZNWvWEeM+LFiwgNdff5158+ahqvTq1YuzzjqLxMTEgLsVN1WfqvJuegZT0jexcONuVKFVck1u6ncKgzo3omvTOtarqYmgBOGR1NRUtm/fzubNm8nKyiIxMZFGjRpx1113MWfOHKpVq0ZmZibbtm2jUaNGfvcxZ84cbr/9dgC6du1K165dDy6bOnUqL7/8MkVFRWzZsoWVK1cetrysb7/9liuuuOJgr7JXXnkl33zzDZdddlnA3Yqbqm1vfiH3vreU/y7fSvtGCdx13qkM6tyItg1q2T0K5jCRkyCO8Us/mK666iree+89tm7dytChQ5kwYQJZWVksWLCAmJgYWrZs6beb72P55ZdfeOqpp5g/fz6JiYmMHDnyhPZTKtBuxU3VtTwzmzETFpK5J48HLurATf1aWVIw5bKCxUowdOhQJk+ezHvvvcdVV11FdnY2DRo0ICYmhlmzZvHrr78edfv+/fszceJEAJYvX87SpUsB2Lt3LzVr1qROnTps27aN//73vwe3Ka+b8X79+vHRRx+Rm5vL/v37+fDDD+nXr18FvlsTilSVd374lStf+J4DRSVMGdWbm/ufYsnBHFXkXEF4qFOnTuzbt4+mTZvSuHFjrrvuOi699FK6dOlCWloa7du3P+r2o0eP5oYbbqBDhw506NCBHj16AHDaaaeRmppK+/btadasGX379j24zahRoxg0aBBNmjRh1qxZB+d3796dkSNH0rNnT8CppE5NTbXipDCWU1DE/R8s49Mlmznr1Po8M7Qb9WpW9zosUwVYd9/muNnnWnWs3rqXMe8sZMPO/fxxYDtGn9XaKp/NYay7b2MiTGkrpYc+Xk7tGjFMuKk3fVonHXtDY3xYgjAmzOQeKOKhj1bw/sIM+rZJ4tmhqdRPsDudzfEL+wShqlYRV4HCpUgyXK3fvo/R7yxkfVYOdwxoy+0D2tqYC+aEhXWCiIuLY+fOnSQlJVmSqACqys6dO4mLi/M6FOPHh4sy+PMHy4mvHsVbv+tJv7b1vQ7JVHFhnSBSUlLIyMggKyvL61DCRlxcHCkpKV6HYXzkFxbzyKcrmPTjJnq2cjrSa1jbkrg5eWGdIGJiYmjVqpXXYRgTND9n5XDLxEWs2rKXMWe35u7zT7V+k0yFCesEYUw4+2zpZu57fxnRUcLrI0/nnPYNvA7JhBlLEMZUMQVFxTz+n1W8NfdXujevyz+v7U5TG8HNBIElCGOqkE27chkzYSHLMrO5uV8r/jSoPTFWpGSCxBKEMVXE9BVbuefdJQjw8vU9GNjJf++/xlSUoP70EJFBIrJGRNaLyH1+lrcQkS9FZKmIzBaRFJ9lxSKy2H18Esw4jQllB4pKePSzlfz+7QW0Sq7Jf27vZ8nBVIqgXUGISBTwPHA+kAHMF5FPVHWlz2pPAW+p6psici7wV+B6d1meqnYLVnzGVAWZe/K4deJCFm3cw8gzWnL/Re2JjbYhP03lCGYRU09gvar+DCAik4HBgG+C6Ajc7b6eBXwUxHiMqVK+Wr2Nu6cuoahYef7a7lzctbHXIZkIE8wipqbAJp/pDHeeryXAle7rK4AEESntUSxORNJF5AcRudzfAURklLtOut0MZ8JFUXEJ/zttNb97I50mdWrw6W1nWnIwnvC6kvoe4F8iMhKYA2QCxe6yFqqaKSKnAF+JyDJV/cl3Y1V9GXgZnO6+Ky9sY4Jja3Y+t09axI8bdjGsZ3PGXtqRuBgrUjLeCGaCyASa+UynuPMOUtXNuFcQIlIL+I2q7nGXZbrPP4vIbCAVOCxBGBNO5qzN4q4pi8krLObZod24PLXsBbcxlSuYRUzzgbYi0kpEqgPXAIe1RhKRZBEpjeF+YLw7P1FEYkvXAfpyeN2FMWGjuER5esZaRrz+I0m1qvPJrWdacjAhIWhXEKpaJCK3AtOBKGC8qq4QkXFAuqp+ApwN/FVEFKeI6RZ38w7Av0WkBCeJPVmm9ZMxYWH7vnzunLyY73/ayZAeKTw6uDM1qluRkgkNYT3kqDGhbO5PO7l98iL25RcybnBnrk5rduyNjKlgNuSoMSGkpER5YfZ6np6xlpbJNXnnxl60a5TgdVjGHMEShDGVaGdOAXdNXcKctVkM7taEJ67oQs1Y+zc0ocm+mcZUkvQNu7h14iJ25R7g8Ss6c23P5jbSoQlpliCMCbKSEuWVb37mb9PXkJJYgw9Gn0HnpnW8DsuYY7IEYUwQZecW8sd3FzNz1XYu6tKIJ3/TldpxMV6HZUxALEEYEyQrNmcz+p2FbMnOY+ylHRl5RksrUjJViiUIY4LgvQUZPPDhMhLjqzN5VB96tEj0OiRjjpslCGMqUEFRMY98upKJ8zbS55Qk/nltKsm1Yr0Oy5gTYgnCmAqSuSePMe8sYElGNn84qzX3DDyVaBsO1FRhliCMqQBz1mZxx+RFFBUr/76+BxfYiG8mDFiCMOYklJQoz89az9Mz13JqgwReur4HrZJreh2WMRXCEoQxJyg7t5C7py7my9XbubxbE564sgvx1e1fyoQP+zYbcwJ8m7COG9yJ63u3sCasJuxYgjDmOL2bvokHP1pOYnx1pvy+D92bWxNWE54sQRgToPxCpwnrpB83ckbrJJ4bZk1YTXizBGFMADJ25zJmwkKWZmQz+uzW/PF8a8Jqwp8lCGOOYc7aLG6fvIhia8JqIowlCGPKUVKi/GvWep6ZuZZ2DRN4cbg1YTWRxRKEMX5k5xZy19TFfLV6O1ekNuXxKzpbE1YTcewbb0wZyzOzGT1hAVuz860Jq4loliCM8TE1fRMPfbScejWtCasxliCMobQJ6wom/biJM1on8c9hqSRZE1YT4SxBmIiXsTuX0e8sZFmmNWE1xpclCBPRvnZ7YS0uVl6+vgcDrQmrMQdZgjARyZqwGnNsliBMxNmTe4C7pixm1posrkhtyhNXdKFG9SivwzIm5FiCMBFleWY2f3hnAdv25vPo4E4MtyasxpTLEoSJGFPdXliTrAmrMQGxBGHCnm8T1r5tknjuGmvCakwgLEGYsLZpl9ML67LMbMac3Zo/DmxHVDUrUjImEJYgTNiavWY7d05ZbE1YjTlBliBM2CkpUf751Xqe/dJpwvrS8B60tCasxhw3SxAmrOzJPcCdUxYze00WV6Y25XFrwmrMCbMEYcLGYU1YL+/M8F7NrQmrMSfBEoQJC9+t38FNb6ZTNz6Gqb/vQ6o1YTXmpAW1RzIRGSQia0RkvYjc52d5CxH5UkSWishsEUnxWTZCRNa5jxHBjNNUbbNWb+eGN+bTIimeT24905KDMRUkoAQhIh+IyMUiEnBCEZEo4HngQqAjMExEOpZZ7SngLVXtCowD/upuWw8YC/QCegJjRcT+680Rpi3fwqi302nXMIFJN/emfoLd32BMRQn0hP8CcC2wTkSeFJF2AWzTE1ivqj+r6gFgMjC4zDodga/c17N8ll8AzFDVXaq6G5gBDAowVhMhPl6cyS0TF9E1pS4Tbu5FYs3qXodkTFgJKEGo6kxVvQ7oDmwAZorI9yJyg4jElLNZU2CTz3SGO8/XEuBK9/UVQIKIJAW4LSIySkTSRSQ9KysrkLdiwsTU+Zu4c8piTm+ZyFu/60ntuPK+hsaYE3U8RUZJwEjgJmAR8A+chDHjJI5/D3CWiCwCzgIygeJAN1bVl1U1TVXT6tevfxJhmKrkrbkb+NP7S+nftj5v3NCTmrHW1sKYYAjoP0tEPgTaAW8Dl6rqFnfRFBFJL2ezTKCZz3SKO+8gVd2MewUhIrWA36jqHhHJBM4us+3sQGI14e3lOT/xxOerOb9jQ/51bSqx0XaPgzHBEuhPr+dUdZa/BaqaVs4284G2ItIKJzFcg1OPcZCIJAO7VLUEuB8Y7y6aDjzhUzE90F1uIpSq8tyXzgA/l3RtzDNDuxFjw4IaE1SB/od1FJG6pRMikigiY462gaoWAbfinOxXAVNVdYWIjBORy9zVzgbWiMhaoCHwuLvtLuBRnCQzHxjnzjMRSFX52/Q1PDNzLUN6pPCPa1ItORhTCURVj72SyGJV7VZm3iJVTQ1WYMcrLS1N09PLK+0yVZWq8sinK3nj+w0M792ccZd1ppr1xmpMhRGRBeWVBAVaxBQlIqJuNnHvcbA2hSaoSkqUBz5axqQfN3Hjma148OIO1nWGMZUo0AQxDadC+t/u9O/decYERVFxCX96bykfLMrk1nPa8MeBp1pyMKaSBZog7sVJCqPd6RnAq0GJyES8A0Ul3DllEZ8v28o9A0/l1nPbeh2SMREpoAThtjJ60X0YEzT5hcXcOnEhM1dt58GLO3BTv1O8DsmYiBXofRBtcfpJ6gjElc5XVfvvNRUm70Axo95O55t1O3js8s4M793C65CMiWiBthV8HefqoQg4B3gLeCdYQZnIk1NQxIjXf+S79Tt46qrTLDkYEwICTRA1VPVLnGaxv6rqw8DFwQvLRJLsvEKGvzqPBb/u5h/XpDKkR8qxNzLGBF2gldQFblff60TkVpw7o2sFLywTKXbtP8D1r81j3bYcXryuOwM7NfI6JGOMK9AriDuAeOB2oAcwHLBBfMxJ2b4vn2tensv67Tm8/NselhyMCTHHvIJwb4obqqr3ADnADUGPyoS9zXvyuO7VeWzbm8/rN5zOGa2TvQ7JGFPGMROEqhaLyJmVEYyJDJt25TLslR/Izi3k7Rt70qNFPa9DMsb4EWgdxCIR+QR4F9hfOlNVPwhKVCZs/ZyVw7WvzCO/qJiJN/emS0odr0MyxpQj0AQRB+wEzvWZp4AlCBOwNVv3cd2r8wBl0s296dC4ttchGWOOItA7qa3ewZyU5ZnZXP/aPKpHV2PCTX1o08AawRkT6gK9k/p1nCuGw6jq7yo8IhN2Fm7czYjxP1I7LoaJN/eiRVJNr0MyxgQg0CKmz3xexwFXAJsrPhwTbn74eSc3vjGf+gmxTLi5N03r1vA6JGNMgAItYnrfd1pEJgHfBiUiEzbmrM1i1NvpNEuMZ8JNvWhQO+7YGxljQkagVxBltQUaVGQgJrzMXLmNMRMW0rpBLd65sSdJtWK9DskYc5wCrYPYx+F1EFtxxogw5gj/WbqFOyYvolPTOrx1Q0/qxMd4HZIx5gQEWsSUEOxATHj4YGEG97y7hB4tEhk/8nQS4iw5GFNVBdQXk4hcISJ1fKbrisjlQYvKVEkT523kj+8uoU/rJN78XU9LDsZUcYF21jdWVbNLJ1R1DzA2KBGZKmn8t7/w5w+XcU67Brw24nTiq59o9ZYxJlQE+l/sL5HYGcAA8MLs9fxt2hou7NyIf1yTSvXoQH93GGNCWaAn+XQReRp43p2+BVgQnJBMVaGqPDNjLc99tZ7B3Zrw96tOIzrKkoMx4SLQ/+bbgAPAFGAykI+TJEyEUlX++t/VPPfVeoamNePpq7tZcjAmzATaimk/cF+QYzFVREmJMvaTFbz9w6+M6NOCsZd2olo18TosY0wFC7QV0wwRqesznSgi04MWlQlZqsr9Hyzj7R9+5ff9T+Hhyyw5GBOuAq2DSHZbLgGgqrtFxO6kjkD/+mo9U9I3cdu5bbj7/FMRseRgTLgKtNC4RESal06ISEv89O5qwtt/l23h7zPWcmVqU0sOxkSAQK8gHgC+FZGvAQH6AaOCFpUJOcszs7l76hJSm9fliSu7WHIwJgIEWkk9TUTScJLCIuAjIC+IcZkQsn1fPqPeSqdufAz/vr4HcTFRXodkjKkEgXbWdxNwB5ACLAZ6A3M5fAhSE4byC4v5/dsL2J1byLt/6EODBOuy25hIEWgdxB3A6cCvqnoOkArsCVZQJjSoKn/+cBmLNu7h6atPo3PTOsfeyBgTNgJNEPmqmg8gIrGquhpoF7ywTCj495yf+WBhJneffyoXdmnsdTjGmEoWaILIcO+D+AiYISIfA78eayMRGSQia0RkvYgccaOdiDQXkVkiskhElorIRe78liKSJyKL3cdLgb8lUxFmrtzG/05bzSVdG3PbuW28DscY44FAK6mvcF8+LCKzgDrAtKNtIyJROH03nQ9kAPNF5BNVXemz2oPAVFV9UUQ6Ap8DLd1lP6lqt0DfiKk4a7bu447Ji+jcpA7/N+Q0a7FkTIQ67h5ZVfXrAFftCaxX1Z8BRGQyMBjwTRAK1HZf1wE2H288pmLtzCngxjfnUzM2mld+m0aN6tZiyZhIFcze1ZoCm3ymM9x5vh4GhotIBs7Vw20+y1q5RU9fi0g/fwcQkVEiki4i6VlZWRUYemQ6UFTC6AkLydpXwMu/TaNRHWuxZEwk87r7zWHAG6qaAlwEvC0i1YAtQHNVTQXuBiaKSO2yG6vqy6qapqpp9evXr9TAw42q8tBHy/nxl138bUhXujWr63VIxhiPBXPQn0ygmc90ijvP143AIABVnSsicTj9Pm0HCtz5C0TkJ+BUID2I8Ua017/bwJT0Tdx6ThsGdyt7oWdMBCs6ADvXw/aVEB0HHS7xOqJKE8wEMR9oKyKtcBLDNcC1ZdbZCAwA3hCRDkAckCUi9YFdqlosIqcAbYGfgxhrRPt6bRaP/WclAzs25O7zT/U6HGO8UVIMuzc4iWD7qkPPO9dDSZGzTrPeliAqgqoWicitwHQgChivqitEZByQrqqfAH8EXhGRu3AqrEeqqopIf2CciBQCJcAfVHVXsGKNZOu353DrxIWc2jCBZ4Z2s667w0lxIRzYD4W5cCAXCveXec6F6rWgTgrUaQbx9SASWqypwt7Mw5PA9pWQtQaK8t2VBBJbQIOO0P5i57lBB0iKrCbfohoenbKmpaVperqVQB2PPbkHuPz578gpKOKjW/qSkhh/5EqqkJ0BmxfB5oXO87YVcNa90PPmyg86nKhCUYF7At9f5rnMiTzg5T7zSwqPL56YeDdZpBxKGnWaHZqu3RSiqwfnswiWnKwjrwiyVkPB3kPrJDRxTv4NOhxKBPXbQfWa3sVdiURkgaqm+VsWzCImE8IKi0u4ZeJCNu/JZ9KoXoeSw94tbjLweeTucJZVi4aGnaBGPfhyHHS6Amome/cmqgJV+OpR+GXOoZO574lcS45vfzHxzqN6PMTUdJ/joXYTP/N9llev6X+7gr3OD4CDj02wZxNsXQ77t5c5uEBCo6MnkRqJ3lyF5GfD9tVHJoPS7y44sTXoBF2H+iSD9s5845cliAj16GcrWbX+F17rr/TY8Cp87yaDfVucFaQa1O8Apw6CpqnQJNX554qJcy7FX+gDs5+Ei5/y9o2EurXT4Zu/Q9MekNjy2Cfsg/P9LI+uAdWC0PCwSar/+YX5TlFMaeLwfd66DNb816dIxhVT00kUdZuVSSLuc+0mEBVz4rEW5jnfv8OKh1bB3oxD61Sv5SSAdhceuiJo0BFqNYiMIrQKZEVMkSJvN2xeDJsX8evy74jaupgUKf11JZDcFpp0d04WTVKhURfnpFSez+6GBW/AmB+gvlVs+1V0AF7sAwiMmXtyJ8ZQpAr7dxyZPA4+Z8D+svcnCSQ0PpQ86pa5AqmTAnF1nUrh0pZDpUlg+0rY9QsHxyqLinW+e75JoEEHqJ0SnEQapqyIKdLk74WtSyFz4aFiot2/HFys2oiMmp1p0ncA1Zp2h0ZdIe6I20yO7uz7YelUmPEXuHZyBb+BMJH+mnOSu3Zq+CUHcH6N16rvPJp2979OYR5kZx6eNLI3OY8ti2H1Z1B84PBtqic4VyaldSgSBUmtnR8tvsVDia0gyk5hwWSfblV3INdJBr51BjvWcfBXVp3m0KQbdP8tW2t1YMjHedSok8QHY86gWtxJnLRq1Yd+d8OXjzjl6636V8S7CR+5u2D2X+GUc6DtQK+j8U5MDUhu4zz8KSlx6glK6z5Kk0hMnE/LobbOtKl0liCqksJ8pwXR5oUHi4vIWnWoojOhsVM81OWqQ0VFbiXy3vxCrnv+O/ZXi2LiiNNJOJnkUKr3GEgfD9MfgFFf22W9r9lPQsE+uOAJK/c+mmrVnLqBWg2cehoTUixBhKrCfKc53pbFh64Mtq08dNkdn+wkgPYXO5f3jbtBbf9jNhSXKLdNXMSvO3N5+8ZeNE86St3C8YiJgwFj4YObYOlk6Fb2PsgIlbUG5r8KPUZCw45eR2PMCbME4TVV2LfVuTLYtsxpXrhtBexYC1rsrBNX10kGZ9x26MqgTkrAv0z/+vkqvl6bxRNXdKFP66SKjb/zb+CHF+DLR6Hj5Uev2I4UXzzotEI65wGvIzHmpFiCqExFBc6vy23L3UTgPnJ3HlqnTjPnXoP2FzvPTbo5lXEnWEwxZf5GXv32F0ae0ZJrezWvmPfhq1o1pxjl9UEw919w1p8q/hhVybqZsO4LGPiY3SNiqjxLEMGyb5tzRbBtxaFksGPtoT5douPcttoXOa0zGnZyb0KruJt2fvxlFw9+tJwz2yTz4MUdKmy/R2jRBzpcCt8+C91/69xMFYmKi+CLB5yE3nOU19EYc9IsQZysogOwY42bCJa5VwUrDm//XbspNOzs3LjTsBM07OI026sWvMF4Nu3K5Q/vLKBZYjzPX9ud6KggVyCf9wismQazHofL/hncY4WqBa879UZDJ0B0rNfRGHPSLEEcj5ztZYqHVjhFRqUVx1GxzlXBqRc4CaFhZychxNer3DALirj5rXSKikt4dUQadeIroQ1+Umunb6Z5L0GvPzjvO5Lk7YZZT0DLfk7xoDFhwBKEP8WFTnHQ1uWHFxP59k2T0AQadYa25zuJoFEXqNfa8xt3SkqUOycvZt32HN644XROqV+r8g7e/39g8QT44iG4/oPKO24omPOUkySsWasJI5YgCvNh41y3FZF7dZC1usxVQXufROBeGVTyVUGg/u+LNcxctY2HL+1Iv7aVPMpefD3o/yenHH79TGhzXuUe3ys7f4J5/4bU4dC4q9fRGFNhLEEU7IW3L3deJzR2ikbaDDhUcZzU1vOrgkB9uCiDF2f/xLCezRlxRktvguh5M8x/xbmKOOWcoNazhIwvHnLqHM59yOtIjKlQVePMF0y1GsCIz5zb+mtW8D0ClWjhxt3c+/4yerWqxyOXdUK8KuaIjnUqrN8dAYvedm4WC2c/z4Y1/3FuGExo6HU0xlQo6xsBoFW/Kp0cNu/JY9RbC2hUO46XhvegerTHf9aOg52hGb963OluIlyVFDvdjNRt7nQ7YkyYsQRRxeUecFos5RcW8+qINBJrhsCIXyJwweNOpf53//A6muBZ9LZTb3X+OOtMzoQlSxBVWEmJcs+7S1i5ZS/PDevGqQ0TvA7pkJQ0pxuO7//ldPccbvL3wlePQfM+ThcjxoQhSxBV2D++XMfny7Zy/4XtObd9CJZ/Dxjr9DT71aNeR1Lxvvm7czOkNWs1YcwSRBX12dLN/OPLdfymewo39zvF63D8S2wBvf8ASyY53ZOHi12/OB0Unjas/IFyjAkDliCqoGUZ2dzz7hJ6tEjkiSs7e9diKRD9/gjxSU4Pp2EyvC0zx0K1aBjwF68jMSaoLEFUMdv35nPzW+kk1YzlpeE9iI0O8fsM4uo4w5Nu+MYZ5L6q2/AdrPwY+t4JtZt4HY0xQWUJogrJLyzm5rfSyc4r5JXfplE/oYp0CNdjpHPD4YyHnG5MqqqSEph+v9P54hm3eR2NMUFnCaKKUFXufX8pSzKyeWZoNzo2qe11SIGLioGBj8LO9ZD+utfRnLglk2DLEudGQBsYyUQASxBVxAuzf+LjxZu5Z+CpDOpcBcdbOHWQ09Pp7L9C3h6vozl+BTnw5SPQNA26DPE6GmMqhSWIKuCLFVv5v+lruOy0JtxyThuvwzkxpTfP5e12mohWNd89CznbYNCT1qzVRAxLECFu5ea93DllMael1OFvQ7qGdoulY2l8mtM0dN5LsHuD19EEbs8m+P6f0HkINDvd62iMqTSWIELYjpwCbn4rnYS4aF7+bRpxMSHeYikQAx4CiYIvx3kdSeBmPuw8n/ewl1EYU+ksQYSogqJi/vD2AnbkFPDKb9NoWDtM+vqp3cRpAbT8fchI9zqaY9s4D5a/58Rct5nX0RhTqSxBhKjs3EJyCop46qrT6JpS1+twKlbf26FmA5j+59C+ea60WWutRs59D8ZEGEsQIapB7Tg+ve1MLj0tDG/Gik2Acx+ATfOcm85C1fL3IHMBnDcWYitx6FZjQoQliBAWExXGf57U651BmmaOhaICr6M50oFcp+6hcTfoeo3X0RjjiTA+A5mQVi3KuXlu9wb48RWvoznS9/+EvZkw6K9Qzf5NTGSyb77xTpvzoPUAmPM3yN3ldTSH7N3s3PfQcTC0OMPraIzxTFAThIgMEpE1IrJeRO7zs7y5iMwSkUUislRELvJZdr+73RoRuSCYcRoPDXzMGZb06795HckhX46DkiJnpDhjIljQEoSIRAHPAxcCHYFhItKxzGoPAlNVNRW4BnjB3bajO90JGAS84O7PhJuGHZ36iPmvwM6fvI7GqZReMskZYzqxpdfRGOOpYF5B9ATWq+rPqnoAmAwMLrOOAqW9ztUBNruvBwOTVbVAVX8B1rv7M+HonAcgOg5meDy+gipM+zPUrO+MY2FMhAtmgmgKbPKZznDn+XoYGC4iGcDnQGkfyoFsi4iMEpF0EUnPysqqqLhNZUto6NxnsPozZ7wFr6z4EDb9AOc+BHFVqLdcY4LE60rqYcAbqpoCXAS8LSIBx6SqL6tqmqqm1a9fP2hBmkrQ5xZIaAJfPODcoFbZCvNhxlho2AVSh1f+8Y0JQcFMEJmAb98EKe48XzcCUwFUdS4QByQHuK0JJ9XjnSE8Ny9yblCrbD88D9kbYdATThNcY0xQE8R8oK2ItBKR6jiVzp+UWWcjMABARDrgJIgsd71rRCRWRFoBbYEfgxirCQVdhzo9vs58BArzKu+4+7bBN09Du4uhVf/KO64xIS5oCUJVi4BbgenAKpzWSitEZJyIXOau9kfgZhFZAkwCRqpjBc6VxUpgGnCLqhYHK1YTIqpVg4GPw94M+OGFyjvuV486d3MPfLTyjmlMFSAayp2lHYe0tDRNT68CvYOaY5s0DH75Bm5fBLWCXLe0ZQn8+yynDuSCx4N7LGNCkIgsUNU0f8u8rqQ25kjnj4OiPJj9RHCPU9qstUYi9P+f4B7LmCrIEoQJPcltIe13sOBN2L46eMdZ/Rn8+q3Ts2yNusE7jjFVlCUIE5rOug+q1wzezXNFBfDFg1C/A3QfGZxjGFPFWYIwoalmknM387rp8PPsit//vH87Pcle8DhERVf8/o0JA5YgTOjq9Qeo0xymPwglFdiILScL5vwftB0IbQZU3H6NCTOWIEzoiolzRnPbtszpQK+izH4CDux3mtQaY8plCcKEts6/gaZp8OWjzkn9ZG1bAQvegNNvgvqnnvz+jAljliBMaBOBC56AnK3OKG8nQxWm/xlia8PZRwxPYowpwxKECX3Nezmju333D9i75cT3s9at8D77PoivV2HhGROuLEGYquG8h6G4EGY9dmLbFx1weopNausULxljjskShKka6p0CvX4PiybA1mXHv336a7BzvdusNabi4zMmDFmCMFVH/3ucO56/eNCpTwhU7i6Y/Vc45RynaasxJiCWIEzVUSMRzrrXqUdYNyPw7WY/CQX7nMpukaCFZ0y4sQRhqpa0G53ipi8ehOKiY6+ftQbmvwo9RkLDjkEPz5hwYgnCVC3R1Z3eXnesgYVvHnv96Q84fTqd80DwYzMmzFiCMFVP+0ug+Rkw6wnI31v+eutmwvoZTlfeNZMrLz5jwoQlCFP1iDitkXJ3wLfP+F+nuMhp1prYymn9ZIw5bpYgTNXUtDt0udoZmnTPpiOXL3gdslbDwMcgOrby4zMmDFiCMFXXAHesiK/KjCWdt9spfmrZD9pfXPlxGRMmLEGYqqtuM+g9BpZOgcyFh+bPecpJEtas1ZiTYgnCVG1n3gXxyYduntuxHua9BKnDoXFXr6MzpkqzBGGqtrjacM798Ot3sPo/MOMhiI6Dcx/yOjJjqjxLEKbq6z4SktvBp7fDms+h392Q0NDrqIyp8ixBmKovKhoGPgq5O50hSnvf4nVExoQFG63dhIe2A507rFv0dYYqNcacNEsQJjyIQN87vI7CmLBiRUzGGGP8sgRhjDHGL0sQxhhj/LIEYYwxxi9LEMYYY/yyBGGMMcYvSxDGGGP8sgRhjDHGL1FVr2OoECKSBfx6ErtIBnZUUDgVyeI6PhbX8bG4jk84xtVCVev7WxA2CeJkiUi6qqZ5HUdZFtfxsbiOj8V1fCItLitiMsYY45clCGOMMX5ZgjjkZa8DKIfFdXwsruNjcR2fiIrL6iCMMcb4ZVcQxhhj/LIEYYwxxq+ITxAiMkhE1ojIehG5z+t4SonIeBHZLiLLvY6llIg0E5FZIrJSRFaISEiM0CMicSLyo4gsceN6xOuYfIlIlIgsEpHPvI7Fl4hsEJFlIrJYRNK9jqeUiNQVkfdEZLWIrBKRPiEQUzv3cyp97BWRO72OC0BE7nK/98tFZJKIVNiQihFdByEiUcBa4HwgA5gPDFPVlZ4GBohIfyAHeEtVO3sdD4CINAYaq+pCEUkAFgCXe/15iYgANVU1R0RigG+BO1T1By/jKiUidwNpQG1VvcTreEqJyAYgTVVD6sYvEXkT+EZVXxWR6kC8qu7xOKyD3PNGJtBLVU/m5tyKiKUpzve9o6rmichU4HNVfaMi9h/pVxA9gfWq+rOqHgAmA4M9jgkAVZ0D7PI6Dl+qukVVF7qv9wGrgKbeRgXqyHEnY9xHSPzyEZEU4GLgVa9jqQpEpA7QH3gNQFUPhFJycA0AfvI6OfiIBmqISDQQD2yuqB1HeoJoCmzymc4gBE54VYGItARSgXkehwIcLMZZDGwHZqhqSMQFPAv8CSjxOA5/FPhCRBaIyCivg3G1ArKA191iuVdFpKbXQZVxDTDJ6yAAVDUTeArYCGwBslX1i4raf6QnCHMCRKQW8D5wp6ru9ToeAFUtVtVuQArQU0Q8L5YTkUuA7aq6wOtYynGmqnYHLgRucYs1vRYNdAdeVNVUYD8QSnWD1YHLgHe9jgVARBJxSj1aAU2AmiIyvKL2H+kJIhNo5jOd4s4z5XDL+N8HJqjqB17HU5ZbHDELGORxKAB9gcvcsv7JwLki8o63IR3i/vpEVbcDH+IUuXotA8jwuQJ8DydhhIoLgYWqus3rQFznAb+oapaqFgIfAGdU1M4jPUHMB9qKSCv3l8E1wCcexxSy3Mrg14BVqvq01/GUEpH6IlLXfV0Dp9HBak+DAlT1flVNUdWWON+tr1S1wn7dnQwRqek2NMAtwhkIeN5iTlW3AptEpJ07awDgeaMRH8MIkeIl10agt4jEu/+fA3DqBitEdEXtqCpS1SIRuRWYDkQB41V1hcdhASAik4CzgWQRyQDGqupr3kZFX+B6YJlb3g/wZ1X93LuQAGgMvOm2LqkGTFXVkGpSGoIaAh865xSigYmqOs3bkA66DZjg/mj7GbjB43iAg4n0fOD3XsdSSlXnich7wEKgCFhEBXa7EdHNXI0xxpQv0ouYjDHGlMMShDHGGL8sQRhjjPHLEoQxxhi/LEEYY4zxyxKEMSFARM4Otd5ejbEEYYwxxi9LEMYcBxEZ7o49sVhE/u12EpgjIs+4ffJ/KSL13XW7icgPIrJURD50+81BRNqIyEx3/IqFItLa3X0tn3EQJrh3xhrjGUsQxgRIRDoAQ4G+bseAxcB1QE0gXVU7AV8DY91N3gLuVdWuwDKf+ROA51X1NJx+c7a481OBO4GOwCk4d64b45mI7mrDmOM0AOgBzHd/3NfA6V68BJjirvMO8IE7rkFdVf3anf8m8K7b/1FTVf0QQFXzAdz9/aiqGe70YqAlzmAwxnjCEoQxgRPgTVW9/7CZIg+VWe9E+68p8HldjP1/Go9ZEZMxgfsSGCIiDQBEpJ6ItMD5PxrirnMt8K2qZgO7RaSfO/964Gt3JL4MEbnc3UesiMRX5pswJlD2C8WYAKnqShF5EGcUtmpAIXALzqA2Pd1l23HqKQBGAC+5CcC3V9LrgX+LyDh3H1dV4tswJmDWm6sxJ0lEclS1ltdxGFPRrIjJGGOMX3YFYYwxxi+7gjDGGOOXJQhjjDF+WYIwxhjjlyUIY4wxflmCMMYY49f/AxofGB7XTP9pAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(1, len(history.history['accuracy'])+1)\n",
    "plt.plot(epochs,history.history['accuracy'],history.history['val_accuracy'])\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['training','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2822c6",
   "metadata": {},
   "source": [
    "# Now, let's save the model since we've take a lot of time to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74855790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 23:17:31.032053: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c5a65",
   "metadata": {},
   "source": [
    "<h3> If you want to load your model you can call it with this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e20f1b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = load_model(\"my_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
